{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6400d3f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T03:57:03.990403Z",
     "start_time": "2025-03-20T03:56:55.070662Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import norm\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7e925c-e164-4957-9f8e-743d00e5dce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "935ec509-2f8d-4c50-acb4-f90cdaccb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba57fee-7745-4497-ae97-003d9e262f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##parameter setting\n",
    "policy = torch.tensor([0.5, 0.5], dtype=torch.float32)\n",
    "policy_new = torch.tensor([0.4, 0.6], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69ba1eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T03:57:07.051758Z",
     "start_time": "2025-03-20T03:57:07.046140Z"
    }
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "###### move one step forward ####\n",
    "#################################\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def step(last_obs):\n",
    "    # last_obs: last observation of state\n",
    "\n",
    "    a=np.random.binomial(1, p=policy[0]*sigmoid(last_obs[0])+policy[1]*sigmoid(last_obs[1]))\n",
    "    mat=np.array([[-0.75*(1-2*a),0],[0,0.75*(1-2*a)]])\n",
    "    z=np.random.multivariate_normal([0,0], [[0.25,0],[0,0.25]])\n",
    "    s_next=mat@last_obs+z\n",
    "    r=np.transpose(s_next)@[2,1]-0.25*(2*a-1)\n",
    "    return(a, r, s_next)\n",
    "\n",
    "\n",
    "def step_new(last_obs):\n",
    "    # last_obs: last observation of state\n",
    "    a=np.random.binomial(1, p=policy_new[0]*sigmoid(last_obs[0])+policy_new[1]*sigmoid(last_obs[1]))\n",
    "    mat=np.array([[-0.75*(1-2*a),0],[0,0.75*(1-2*a)]])\n",
    "    z=np.random.multivariate_normal([0,0], [[0.25,0],[0,0.25]])\n",
    "    s_next=mat@last_obs+z\n",
    "    r=np.transpose(s_next)@[2,1]-0.25*(2*a-1)\n",
    "    return(a, r, s_next)\n",
    "\n",
    "def get_new_probs(state,action):\n",
    "    prob = action*(policy_new[0].item()*sigmoid(state[0])+policy_new[1].item()*sigmoid(state[1]))+(1-action)*(1-(policy_new[0].item()*sigmoid(state[0])+policy_new[1].item()*sigmoid(state[1])))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2221da9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T03:57:09.800005Z",
     "start_time": "2025-03-20T03:57:09.791357Z"
    }
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "#### generate one trajectory ####\n",
    "#################################\n",
    "\n",
    "\n",
    "def gen_traj(T, gam, seed=None, s_init=None):\n",
    "    # seed: random seed\n",
    "    # s_init: initial state\n",
    "    # gam: discount\n",
    "    # T: iterative number\n",
    "\n",
    "    # initialize the state\n",
    "    if seed is None and s_init is None:\n",
    "        s = np.random.multivariate_normal([0,0],[[1,0],[0,1]])\n",
    "    elif seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        s = np.random.multivariate_normal([0,0],[[1,0],[0,1]])\n",
    "    if s_init is not None:\n",
    "        s = s_init\n",
    "\n",
    "    s_traj = [s]\n",
    "    a_traj = []\n",
    "    r_traj = []\n",
    "\n",
    "    ret = 0\n",
    "    for i in range(T):\n",
    "        a, r, s_next = step(s)\n",
    "        s_traj.append(s_next)\n",
    "        a_traj.append(a)\n",
    "        r_traj.append(r)\n",
    "        s = s_next  # update current S as S_next\n",
    "        ret += r * gam**i\n",
    "\n",
    "    ## output state, reward trajectory. return\n",
    "    return [s_traj, a_traj, r_traj, ret]\n",
    "\n",
    "\n",
    "def gen_traj_new(T, gam, seed=None, s_init=None):\n",
    "    # seed: random seed\n",
    "    # s_init: initial state\n",
    "    # gam: discount\n",
    "    # T: iterative number\n",
    "\n",
    "    # initialize the state\n",
    "    if seed is None and s_init is None:\n",
    "        s = np.random.multivariate_normal([0,0],[[1,0],[0,1]])\n",
    "    elif seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        s = np.random.multivariate_normal([0,0],[[1,0],[0,1]])\n",
    "    if s_init is not None:\n",
    "        s = s_init\n",
    "\n",
    "    s_traj = [s]\n",
    "    a_traj = []\n",
    "    r_traj = []\n",
    "\n",
    "    ret = 0\n",
    "    for i in range(T):\n",
    "        a, r, s_next = step_new(s)\n",
    "        s_traj.append(s_next)\n",
    "        a_traj.append(a)\n",
    "        r_traj.append(r)\n",
    "        s = s_next  # update current S as S_next\n",
    "        ret += r * gam**i\n",
    "\n",
    "    ## output state, reward trajectory. return\n",
    "    return [s_traj, a_traj, r_traj, ret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "149bfe6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T03:57:10.570183Z",
     "start_time": "2025-03-20T03:57:10.553870Z"
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "#### generate data ####\n",
    "#######################\n",
    "\n",
    "\n",
    "def data_gen(N, T_obs, T, gam, seed=None, s_init=None):\n",
    "    # N: number of trajectories\n",
    "    # T_obs: observed stage numbers\n",
    "\n",
    "    s_data = np.zeros((N, T_obs, 2))\n",
    "    a_data = np.zeros((N, T_obs), dtype=int)\n",
    "    r_data = np.zeros((N, T_obs))\n",
    "    ret_data = []\n",
    "\n",
    "\n",
    "    for i in range(N):\n",
    "        if seed is not None:\n",
    "            seed += 1\n",
    "        tmp = gen_traj(T, gam, seed, s_init)\n",
    "        s_data[i] = tmp[0][0:T_obs]  # store the i-th state trajectory\n",
    "        a_data[i] = tmp[1][0:T_obs]\n",
    "        r_data[i] = tmp[2][0:T_obs]  # store the i-th reward trajectory\n",
    "        ret_data.append(tmp[3])\n",
    "  \n",
    "\n",
    "    ## output observed state, reward trajectory and true return\n",
    "    return [s_data, a_data ,r_data, ret_data]\n",
    "\n",
    "def data_gen_new(N, T_obs, T, gam, seed=None, s_init=None):\n",
    "    # N: number of trajectories\n",
    "    # T_obs: observed stage numbers\n",
    "\n",
    "    s_data = np.zeros((N, T_obs, 2))\n",
    "    a_data = np.zeros((N, T_obs), dtype=int)\n",
    "    r_data = np.zeros((N, T_obs))\n",
    "    ret_data = []\n",
    "\n",
    "    for i in range(N):\n",
    "        if seed is not None:\n",
    "            seed += 1\n",
    "        tmp = gen_traj_new(T, gam, seed, s_init)\n",
    "        s_data[i] = tmp[0][0:T_obs]  # store the i-th state trajectory\n",
    "        a_data[i] = tmp[1][0:T_obs]\n",
    "        r_data[i] = tmp[2][0:T_obs]  # store the i-th reward trajectory\n",
    "        ret_data.append(tmp[3])\n",
    "\n",
    "    ## output observed state, reward trajectory and true return\n",
    "    return [s_data, a_data ,r_data, ret_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1819d7-7a2c-4a0c-ba03-be4fb1712cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, num_quantiles, hidden_size=32):\n",
    "        super(QuantileNetwork, self).__init__()\n",
    "        self.num_quantiles = num_quantiles\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Feature extraction network\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        ).to(device)\n",
    "        \n",
    "        # Output layer for quantiles\n",
    "        self.quantiles = nn.Linear(hidden_size, action_dim * num_quantiles).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convert input to tensor if needed and move to device\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.FloatTensor(x).to(device)\n",
    "        elif x.device != device:\n",
    "            x = x.to(device)\n",
    "            \n",
    "        features = self.feature(x)\n",
    "        quantiles = self.quantiles(features)\n",
    "        return quantiles.view(-1, self.action_dim, self.num_quantiles)\n",
    "\n",
    "class BehaviorPolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=32):\n",
    "        super(BehaviorPolicyNetwork, self).__init__()\n",
    "        # Policy network architecture\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        ).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convert input to tensor if needed and move to device\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.FloatTensor(x).to(device)\n",
    "        elif x.device != device:\n",
    "            x = x.to(device)\n",
    "        return self.net(x)\n",
    "    \n",
    "    def get_probs(self, x):\n",
    "        logits = self.forward(x)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "\n",
    "class QTD_Agent:\n",
    "    def __init__(self, state_dim, action_dim, gamma, lr, num_quantiles, \n",
    "                 behavior_lr, beta, batch_size):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.num_quantiles = num_quantiles\n",
    "        self.beta = beta\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize networks and move to GPU\n",
    "        self.net = QuantileNetwork(state_dim, action_dim, num_quantiles).to(device)\n",
    "        self.target_net = QuantileNetwork(state_dim, action_dim, num_quantiles).to(device)\n",
    "        self.target_net.load_state_dict(self.net.state_dict())\n",
    "        \n",
    "        self.behavior_policy = BehaviorPolicyNetwork(state_dim, action_dim).to(device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.behavior_optimizer = optim.Adam(self.behavior_policy.parameters(), lr=behavior_lr)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.buffer = deque(maxlen=100000)\n",
    "        self.batch_size = batch_size\n",
    "        self.quantile_fractions = torch.linspace(0.5/num_quantiles, 1-0.5/num_quantiles, num_quantiles).to(device)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        self.scheduler=SequentialLR(self.optimizer,\n",
    "            schedulers=[\n",
    "                LinearLR(self.optimizer, start_factor=0.2, end_factor=1.0, total_iters=300),  # Warm-up phase\n",
    "                CosineAnnealingLR(self.optimizer, T_max=700, eta_min=1e-5)                  # Main decay phase\n",
    "            ],\n",
    "            milestones=[300]  # Switch after 300 steps\n",
    "                                   )                \n",
    "        self.behavior__scheduler = LinearLR(self.behavior_optimizer, start_factor=1, end_factor=0.2, total_iters=3000)\n",
    " \n",
    "    def get_target_policy_probs(self, states, actions=None):\n",
    "        with torch.no_grad():\n",
    "            # Calculate target policy probabilities based on state components\n",
    "            sigmoid = torch.nn.Sigmoid()\n",
    "            state_components = states  # states shape: [batch_size, 2]\n",
    "            p = policy_new[0] * sigmoid(state_components[:, 0]) + policy_new[1] * sigmoid(state_components[:, 1])\n",
    "            \n",
    "            # Clamp probabilities to [0,1] range\n",
    "            p = torch.clamp(p, 0.0, 1.0)\n",
    "            \n",
    "            # Construct action probability distribution\n",
    "            # probs shape: [batch_size, 2]\n",
    "            # probs[:, 1] = p (probability of action 1)\n",
    "            # probs[:, 0] = 1-p (probability of action 0)\n",
    "            probs = torch.stack([1-p, p], dim=1)\n",
    "            \n",
    "            if actions is not None:\n",
    "                actions = actions.to(device)\n",
    "                return probs.gather(1, actions.unsqueeze(1))  # Return probability of selected action\n",
    "        return probs  # Return full action probability distribution\n",
    "    \n",
    "    # Polyak Averaging for target network update\n",
    "    def update_target(self, tau):\n",
    "        for target_param, param in zip(self.target_net.parameters(), self.net.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done, behavior_prob):\n",
    "        # Ensure we store Python scalars or NumPy arrays\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            state = state.cpu().numpy()\n",
    "        if isinstance(next_state, torch.Tensor):\n",
    "            next_state = next_state.cpu().numpy()\n",
    "        self.buffer.append((state, action, reward, next_state, done, behavior_prob))\n",
    "    \n",
    "    def train_behavior_policy(self, states, actions):\n",
    "        probs = self.behavior_policy.get_probs(states)\n",
    "        action_probs = probs.gather(1, actions.unsqueeze(1))\n",
    "        loss = -torch.log(action_probs).mean()\n",
    "        \n",
    "        self.behavior_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.behavior_optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def pseudo_sample_next_actions(self, next_states):\n",
    "        probs = self.get_target_policy_probs(next_states)\n",
    "        probs = torch.clamp(probs, min=1e-5, max=1.0-1e-5)\n",
    "        probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "        actions = torch.multinomial(probs, num_samples=1)\n",
    "        return actions.squeeze(-1)\n",
    "        \n",
    "    def get_quantiles_for_state(self, state, action=None):\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            quantiles = self.net(state_tensor).squeeze(0)  # [action_dim, num_quantiles]\n",
    "            \n",
    "            if action is not None:\n",
    "                return quantiles[action].cpu().numpy()\n",
    "            return quantiles.cpu().numpy()\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return 0, 0\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones, behavior_probs = zip(*batch)\n",
    "        \n",
    "        # Convert to tensors and move to GPU\n",
    "        states = torch.FloatTensor(np.array(states)).to(device)\n",
    "        actions = torch.LongTensor(np.array(actions)).to(device)\n",
    "        rewards = torch.FloatTensor(np.array(rewards)).unsqueeze(-1).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "        dones = torch.FloatTensor(np.array(dones)).unsqueeze(-1).to(device)\n",
    "        behavior_probs = torch.FloatTensor(np.array(behavior_probs)).unsqueeze(-1).to(device)\n",
    "        \n",
    "        # 1. Train behavior policy\n",
    "        behavior_loss = self.train_behavior_policy(states, actions)\n",
    "        \n",
    "        # 2. Calculate importance weights\n",
    "        with torch.no_grad():\n",
    "            current_action_probs = self.get_target_policy_probs(states, actions)\n",
    "            importance_weights = (current_action_probs / behavior_probs+ 1e-5).clamp(0, 1/self.beta)\n",
    "        \n",
    "        # 3. Get current quantile estimates\n",
    "        current_quantiles = self.net(states)\n",
    "        actions = actions.view(-1, 1, 1).expand(-1, -1, self.num_quantiles)\n",
    "        current_quantiles = current_quantiles.gather(1, actions).squeeze(1)\n",
    "        \n",
    "        # 4. Compute target quantiles\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.pseudo_sample_next_actions(next_states)\n",
    "            next_actions = next_actions.view(-1, 1, 1).expand(-1, -1, self.num_quantiles)\n",
    "            \n",
    "            target_quantiles = self.target_net(next_states)\n",
    "            target_quantiles = target_quantiles.gather(1, next_actions).squeeze(1)\n",
    "            target_quantiles = rewards + self.gamma * target_quantiles * (1 - dones)\n",
    "        \n",
    "        # 5. Compute quantile regression loss\n",
    "        diff = target_quantiles.unsqueeze(-1) - current_quantiles.unsqueeze(1)\n",
    "        weight = torch.abs(self.quantile_fractions - (diff.detach() < 0).float())\n",
    "        \n",
    "        loss = torch.where(\n",
    "            diff.abs() < 1,\n",
    "            0.5 * diff.pow(2) * weight,\n",
    "            (diff.abs() - 0.5) * weight\n",
    "        )\n",
    "        loss = (loss * importance_weights.unsqueeze(-1)).mean()\n",
    "    \n",
    "        # 6. Optimize model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.net.parameters(), 2.0)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        return loss.item(), behavior_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57548174-9422-46b2-a2cf-685b9e5efff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QTD_new(state_traj,\n",
    "        action_traj,\n",
    "        reward_traj,\n",
    "        state_dim,\n",
    "        action_card,\n",
    "        quantile_num,\n",
    "        gam,\n",
    "        seed,\n",
    "        lr,\n",
    "        behavior_lr,\n",
    "        beta,\n",
    "        batch_size,\n",
    "        tau):\n",
    "\n",
    "    agent = QTD_Agent(state_dim = state_dim, action_dim = action_card,gamma=gam,lr=lr,num_quantiles=quantile_num,behavior_lr=behavior_lr,\n",
    "                      beta=beta,batch_size=batch_size)\n",
    "    n_tr=np.shape(state_traj)[0]\n",
    "    T_obs=np.shape(state_traj)[1]\n",
    "    for i in range(n_tr):\n",
    "        for j in range(T_obs-1):\n",
    "            with torch.no_grad():   \n",
    "                state = state_traj[i,j]\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                action = action_traj[i,j]\n",
    "                behavior_probs = agent.behavior_policy.get_probs(state_tensor)\n",
    "                behavior_prob = behavior_probs[0, action].item()\n",
    "                reward = reward_traj[i,j]\n",
    "                next_state = state_traj[i,j+1]\n",
    "                done=False\n",
    "                behavior_prob = behavior_probs[0, action].item()\n",
    "        \n",
    "            agent.store_transition(state, action, reward, next_state, done, behavior_prob)\n",
    "    for step in range(2001):\n",
    "        loss, behavior_loss = agent.train()\n",
    "        if step % 5 == 0:\n",
    "            agent.update_target(tau=tau)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ae5ff8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T03:57:26.154742Z",
     "start_time": "2025-03-20T03:57:26.150664Z"
    }
   },
   "outputs": [],
   "source": [
    "## calculate V estimator based on QTD output\n",
    "def v_hat_f(state):\n",
    "    q_hat = (np.mean(agent.get_quantiles_for_state(state), axis=1))\n",
    "    v = (q_hat[0]*get_new_probs(state,0)+q_hat[1]*get_new_probs(state,1)).item()\n",
    "    return v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c02206d-9e4e-4aac-be2e-b7ce3ddfadd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_calculation_clip(states, actions, clip):\n",
    "    weight=1\n",
    "    step=np.shape(states)[0]-1\n",
    "    for i in range(step):\n",
    "        state_tensor = torch.FloatTensor(states[i]).unsqueeze(0)\n",
    "        behavior_probs = agent.behavior_policy.get_probs(state_tensor)\n",
    "        behavior_prob = behavior_probs[0, actions[i]].item()\n",
    "        weight*=get_new_probs(states[i],actions[i])/behavior_prob\n",
    "    weight=max(min(clip[1],weight),clip[0]) \n",
    "    return(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfddf9eb-fc8c-4f07-9a67-6bfec1ea4945",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replay buffer+weight estimation\n",
    "def replay_buffer(s_traj, s_traj_train, a_traj, r_traj,step_forward,clip):\n",
    "    n = np.shape(s_traj)[0]\n",
    "    p = np.shape(s_traj)[1] - step_forward\n",
    "    state_dim=np.shape(s_traj)[2]\n",
    "#logistic regression\n",
    "    s_b = s_traj_train[:,1:p].reshape(-1,state_dim)\n",
    "    s_0 = s_traj_train[:,0]\n",
    "    X = np.vstack([s_b,s_0])\n",
    "    y = np.concatenate([np.ones(np.shape(s_b)[0]), np.zeros(np.shape(s_0)[0])])\n",
    "    rt = 1/(p-1)\n",
    "    model = LogisticRegression().fit(X, y)\n",
    "    \n",
    "    Mem_state = np.zeros((n*p,step_forward+1,state_dim))\n",
    "    Mem_action = np.zeros((n*p,step_forward+1),dtype=int)\n",
    "    Mem_reward = np.zeros((n*p,step_forward+1))\n",
    "    idx_weight=[]\n",
    "    state_zero=s_traj[:,0]\n",
    "    for i in range(n):\n",
    "        for j in range(p):\n",
    "            Mem_state[(i*p+j),:] = s_traj[i,j:(j+step_forward+1)]\n",
    "            Mem_action[(i*p+j),:] = a_traj[i,j:(j+step_forward+1)]\n",
    "            Mem_reward[(i*p+j),:] = r_traj[i,j:(j+step_forward+1)]\n",
    "            idx_weight.append(\n",
    "            weight_calculation_clip(Mem_state[(i*p+j), :], Mem_action[(i*p+j), :],clip) * rt*model.predict_proba(np.expand_dims(Mem_state[(i*p+j), 0], axis=0))[0, 1]/model.predict_proba(np.expand_dims(Mem_state[(i*p+j), 0], axis=0))[0, 0]\n",
    "            )\n",
    "\n",
    "    total = np.array(idx_weight).sum()\n",
    "    idx_weight_final=np.array(idx_weight)/total\n",
    "    return([Mem_state,Mem_action,Mem_reward,idx_weight_final])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f08e423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T03:58:54.244086Z",
     "start_time": "2025-03-20T03:58:54.239265Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def weighted_percentile(data, weights, perc,method):\n",
    "\n",
    "    data = np.array(data)\n",
    "    weights = np.array(weights)\n",
    "    idx = np.argsort(data)\n",
    "    data = data[idx] # sort data\n",
    "    weights = weights[idx] # sort weights\n",
    "    cdf = np.cumsum(weights) / np.sum(weights)\n",
    "    count = np.sum([ cdf[i] <= perc for i in range(np.shape(cdf)[0]) ])\n",
    "    #if output=infty return the maximum of V\n",
    "    if data[count]==float('inf') or method==\"min\":\n",
    "        count-=1\n",
    "\n",
    "        \n",
    "    return(data[count])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "758f7b57-0bbe-4fb5-9e95-df19b2d7262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(s_traj, r_traj,step_forward,gam,quan_num):\n",
    "    # s_traj: state trajectory\n",
    "    # r_traj: reward trajectory\n",
    "    # step_forward: number of steps used in approximating return\n",
    "    # gam: discount\n",
    "    if np.shape(s_traj)[1]!=step_forward+1:\n",
    "        print(\"length dismatch\")\n",
    "    if np.shape(s_traj)[0]!=np.shape(r_traj)[0]:\n",
    "        print(\"height dismatch\")\n",
    "    \n",
    "    n = np.shape(s_traj)[0]\n",
    "    u = np.random.randint(0, quan_num - 1, size=n)\n",
    "    sc = list(\n",
    "        map(\n",
    "            abs,\n",
    "            np.sum([gam**i * r_traj[:, i] for i in range(step_forward)],\n",
    "                   axis=0) +\n",
    "            [\n",
    "                gam**step_forward * agent.get_quantiles_for_state(s_traj[i, step_forward])[step_new(s_traj[i, step_forward])[0],u[i]] -\n",
    "                v_hat_f(s_traj[i, 0] ) for i in range(n)\n",
    "            ]))\n",
    "    return (sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40eb6eba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T03:59:33.791316Z",
     "start_time": "2025-03-20T03:59:33.787823Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6175ce8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T03:59:41.448956Z",
     "start_time": "2025-03-20T03:59:41.435957Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_rb_res(data_train, data_test, gam, alp, step_forward, num_quantiles,B,eta,seed,sample_size,clip,action_card,lr,behavior_lr,beta,batch_size,tau):\n",
    "    n_tr, n_te = np.shape(data_train[0])[0], np.shape(data_test[0])[0]\n",
    "    s_init_te = data_test[0]\n",
    "    a_init_te = data_test[1]\n",
    "    ret_te = data_test[3]\n",
    "    state_dim=np.shape(data_test[0])[2]\n",
    "    \n",
    "    ## split training data\n",
    "    idx_perm = np.random.permutation(list(range(0, n_tr)))\n",
    "    idx_tr, idx_cal = [idx_perm[0:int(n_tr / 2)], idx_perm[int(n_tr / 2):n_tr]]\n",
    "    s_train_fold = data_train[0][idx_tr]\n",
    "    a_train_fold = data_train[1][idx_tr]\n",
    "    r_train_fold = data_train[2][idx_tr]\n",
    "    \n",
    "    ## train return distribution using QTD\n",
    "    agent=QTD_new(state_traj=s_train_fold,\n",
    "        action_traj=a_train_fold,\n",
    "        reward_traj=r_train_fold,\n",
    "        state_dim=state_dim,\n",
    "        action_card=action_card,\n",
    "        quantile_num=num_quantiles,\n",
    "        gam=gam,\n",
    "        seed=seed,\n",
    "        lr=lr,\n",
    "        behavior_lr=behavior_lr,\n",
    "        beta=beta,\n",
    "        batch_size=batch_size,\n",
    "        tau=tau)\n",
    "\n",
    "    def v_hat_f(state):\n",
    "        q_hat = (np.mean(agent.get_quantiles_for_state(state), axis=1))\n",
    "        v = (q_hat[0]*get_new_probs(state,0)+q_hat[1]*get_new_probs(state,1)).item()\n",
    "        return v\n",
    "\n",
    "    def weight_calculation_clip(states, actions, clip):\n",
    "        with torch.no_grad():\n",
    "            weight=1\n",
    "            step=np.shape(states)[0]-1\n",
    "            for i in range(step):\n",
    "                state_tensor = torch.FloatTensor(states[i]).unsqueeze(0)\n",
    "                behavior_probs = agent.behavior_policy.get_probs(state_tensor)\n",
    "                behavior_prob = behavior_probs[0, actions[i]].item()\n",
    "                weight*=get_new_probs(states[i],actions[i])/behavior_prob\n",
    "            weight=max(min(clip[1],weight),clip[0])\n",
    "        return(weight)\n",
    "\n",
    "    def replay_buffer(s_traj, s_traj_train, a_traj, r_traj,step_forward,clip):\n",
    "        with torch.no_grad():\n",
    "            n = np.shape(s_traj)[0]\n",
    "            p = np.shape(s_traj)[1] - step_forward\n",
    "            state_dim=np.shape(s_traj)[2]\n",
    "        #logistic regression\n",
    "            s_b = s_traj_train[:,1:p].reshape(-1,state_dim)\n",
    "            s_0 = s_traj_train[:,0]\n",
    "            X = np.vstack([s_b,s_0])\n",
    "            y = np.concatenate([np.ones(np.shape(s_b)[0]), np.zeros(np.shape(s_0)[0])])\n",
    "            rt = 1/(p-1)\n",
    "            model = LogisticRegression().fit(X, y)\n",
    "            \n",
    "            Mem_state = np.zeros((n*p,step_forward+1,state_dim))\n",
    "            Mem_action = np.zeros((n*p,step_forward+1),dtype=int)\n",
    "            Mem_reward = np.zeros((n*p,step_forward+1))\n",
    "            idx_weight=[]\n",
    "            state_zero=s_traj[:,0]\n",
    "            for i in range(n):\n",
    "                for j in range(p):\n",
    "                    Mem_state[(i*p+j),:] = s_traj[i,j:(j+step_forward+1)]\n",
    "                    Mem_action[(i*p+j),:] = a_traj[i,j:(j+step_forward+1)]\n",
    "                    Mem_reward[(i*p+j),:] = r_traj[i,j:(j+step_forward+1)]\n",
    "                    idx_weight.append(\n",
    "                    weight_calculation_clip(Mem_state[(i*p+j), :], Mem_action[(i*p+j), :],clip) * rt*model.predict_proba(np.expand_dims(Mem_state[(i*p+j), 0], axis=0))[0, 1]/model.predict_proba(np.expand_dims(Mem_state[(i*p+j), 0], axis=0))[0, 0]\n",
    "                    )\n",
    "        \n",
    "            total = np.array(idx_weight).sum()\n",
    "            idx_weight_final=np.array(idx_weight)/total\n",
    "        return([Mem_state,Mem_action,Mem_reward,idx_weight_final])\n",
    "    \n",
    "    def scoring(s_traj, r_traj,step_forward,gam,quan_num):\n",
    "    # s_traj: state trajectory\n",
    "    # r_traj: reward trajectory\n",
    "    # step_forward: number of steps used in approximating return\n",
    "    # gam: discount\n",
    "        with torch.no_grad():\n",
    "            if np.shape(s_traj)[1]!=step_forward+1:\n",
    "                print(\"length dismatch\")\n",
    "            if np.shape(s_traj)[0]!=np.shape(r_traj)[0]:\n",
    "                print(\"height dismatch\")\n",
    "            \n",
    "            n = np.shape(s_traj)[0]\n",
    "            u = np.random.randint(0, quan_num - 1, size=n)\n",
    "            sc = list(\n",
    "                map(\n",
    "                    abs,\n",
    "                    np.sum([gam**i * r_traj[:, i] for i in range(step_forward)],\n",
    "                           axis=0) +\n",
    "                    [\n",
    "                        gam**step_forward * agent.get_quantiles_for_state(s_traj[i, step_forward])[step_new(s_traj[i, step_forward])[0],u[i]] -\n",
    "                        v_hat_f(s_traj[i, 0] ) for i in range(n)\n",
    "                    ]))\n",
    "        return (sc)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "    ## calculate nonconformity scores based on test set\n",
    "        sc_te = [abs(ret_te[i] - v_hat_f(s_init_te[i, 0])) for i in range(n_te)]\n",
    "        \n",
    "        ## replay buffer\n",
    "        l = np.shape(step_forward)[0]\n",
    "        if isinstance(eta, int) == False:\n",
    "            m = np.shape(eta)[0]\n",
    "        elif isinstance(eta, int) == True:\n",
    "            m = 1\n",
    "            \n",
    "        PI_cov_e = np.zeros((m,l))\n",
    "        PI_len_e = np.zeros((m,l))\n",
    "                \n",
    "    \n",
    "        for k in range(l):\n",
    "            \n",
    "           \n",
    "            quan_B_e = np.zeros((m,n_te,B))\n",
    "     \n",
    "            for i in range(B):\n",
    "                \n",
    "            #n_cal = np.random.choice(a=[j for j in range(np.shape(Mem[0])[0])], p=p,size=200)\n",
    "                Mem=replay_buffer(s_traj=data_train[0][idx_cal, :], s_traj_train=s_train_fold, a_traj=data_train[1][idx_cal, :], \n",
    "                             r_traj=data_train[2][idx_cal, :],step_forward=step_forward[k],clip=clip)\n",
    "                \n",
    "                weight_is=Mem[-1]\n",
    "                n_cal = np.random.choice(range(np.shape(weight_is)[0]),size=sample_size, p=weight_is)\n",
    "                ## calculate nonconformity scores based on calibration set\n",
    "                sc_rb = scoring(s_traj=Mem[0][n_cal,],\n",
    "                                r_traj=Mem[2][n_cal,],\n",
    "                                step_forward=step_forward[k],\n",
    "                                gam=gam,\n",
    "                                quan_num=num_quantiles)\n",
    "                sc_rb.append(float('inf')) \n",
    "                for j in range(n_te): \n",
    "                    for z in range(m):\n",
    "                        quan_B_e[z][j,i] = weighted_percentile(data=sc_rb,weights=np.ones(sample_size+1),\n",
    "                                                          perc=1-alp*eta[z],method=\"max\")\n",
    "                        \n",
    "            critical_value_rb_e = np.zeros((m,n_te))\n",
    "     \n",
    "            for z in range(m):\n",
    "                critical_value_rb_e[z,:] = [ np.percentile(a=quan_B_e[z][k,:],\n",
    "                                                               q=eta[z]*100) for k in range(n_te) ]\n",
    "    \n",
    "                \n",
    "                PI_cov_e[z,k] = np.mean([sc_te[k] <= critical_value_rb_e[z,k] \n",
    "                                             for k in range(n_te)])\n",
    "                PI_len_e[z,k] = 2 * np.mean(critical_value_rb_e[z,:])\n",
    "                \n",
    "            \n",
    "    return([PI_cov_e,PI_len_e])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02f506fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T15:22:31.005418Z",
     "start_time": "2025-03-19T15:22:31.005418Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantile_region_res(data_train, data_test, gam, alp, num_quantiles,seed,action_card,lr,behavior_lr,beta,batch_size,tau):\n",
    "\n",
    "    n_tr, n_te = np.shape(data_train[0])[0], np.shape(data_test[0])[0]\n",
    "    state_dim = np.shape(data_train[0])[2]\n",
    "    s_init_te = data_test[0]\n",
    "    a_init_te = data_test[1] \n",
    "    ret_te = data_test[3]\n",
    "    quant_num = num_quantiles\n",
    "    ## train QTD using full training data\n",
    "    agent=QTD_new(state_traj=data_train[0],\n",
    "        action_traj=data_train[1],\n",
    "        reward_traj=data_train[2],\n",
    "        state_dim=state_dim,\n",
    "        action_card=action_card,\n",
    "        quantile_num=num_quantiles,\n",
    "        gam=gam,\n",
    "        seed=seed,\n",
    "        lr=lr,\n",
    "        behavior_lr=behavior_lr,\n",
    "        beta=beta,\n",
    "        batch_size=batch_size,\n",
    "        tau=tau)\n",
    "    with torch.no_grad():\n",
    "        data_t=data_test[0].reshape(-1,state_dim)\n",
    "        quant_interval_lower=np.zeros(n_te)\n",
    "        quant_interval_upper=np.zeros(n_te)\n",
    "        ## lower and upper quanitles for each states \n",
    "        for i in range(n_te):\n",
    "            data_aug=np.hstack((agent.get_quantiles_for_state(data_t[i])[0,:], agent.get_quantiles_for_state(data_t[i])[1,:])) \n",
    "            weight_aug=np.hstack((np.ones(quant_num)*get_new_probs(data_t[i],0)/quant_num,np.ones(quant_num)*get_new_probs(data_t[i],1)/quant_num))\n",
    "            quant_interval_lower[i]=weighted_percentile(data_aug,weight_aug,alp/2,method=\"min\")\n",
    "            quant_interval_upper[i]=weighted_percentile(data_aug,weight_aug,1-alp/2,method=\"max\")\n",
    "                           \n",
    "    \n",
    "        ## calculate coverage\n",
    "        t1 = [\n",
    "        ret_te[i] >= quant_interval_lower[i] for i in range(n_te)\n",
    "        ]\n",
    "        t2 = [\n",
    "        ret_te[i] <= quant_interval_upper[i] for i in range(n_te)\n",
    "        ]\n",
    "        quan_PI_cov = np.mean([all([t1[i], t2[i]]) for i in range(n_te)])\n",
    "        quan_PI_len = np.mean(quant_interval_upper - quant_interval_lower\n",
    "            )\n",
    "\n",
    "    return ([quan_PI_cov, quan_PI_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2b737c9-5a7b-44fc-bada-4f763172ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallel Calculation\n",
    "def run_single_experiment(i, n_tr, gam, T_obs, seed, n_te, T, num_quantiles, B, alp, eta, step_forward,sample_size,clip,action_card,lr,behavior_lr,beta,batch_size,tau):\n",
    "\n",
    "    data_train = data_gen(N=n_tr,\n",
    "                          T_obs=T_obs,\n",
    "                          T=T,\n",
    "                          gam=gam,\n",
    "                          seed=seed + i,\n",
    "                          s_init=None)\n",
    "\n",
    "\n",
    "    data_test = data_gen_new(N=n_te,\n",
    "                             T_obs=1,\n",
    "                             T=T,\n",
    "                             gam=gam,\n",
    "                             seed=seed + i + 10000,\n",
    "                             s_init=None)\n",
    "\n",
    "    result = new_rb_res(data_train=data_train,\n",
    "                        data_test=data_test,\n",
    "                        gam=gam,\n",
    "                        alp=alp,\n",
    "                        step_forward=step_forward,\n",
    "                        num_quantiles=num_quantiles,\n",
    "                        B=B,\n",
    "                        eta=eta,\n",
    "                        seed=seed + i,\n",
    "                        sample_size=sample_size,\n",
    "                       clip=clip,\n",
    "                       action_card=action_card,\n",
    "                       lr=lr,\n",
    "                       behavior_lr=behavior_lr,\n",
    "                       beta=beta,\n",
    "                       batch_size=batch_size,\n",
    "                       tau=tau)\n",
    "   \n",
    "    return result  # return [PI_cov_e, PI_len_e]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0866d0-6cad-42ca-bc1e-db033986a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter setting\n",
    "rep = 100\n",
    "n_tr = 200\n",
    "gam = 0.8\n",
    "T_obs = 30\n",
    "seed = 2025\n",
    "n_te = 310\n",
    "T = 70\n",
    "num_quantiles = 30\n",
    "B = 50\n",
    "alp = 0.1\n",
    "eta = [0.2,0.3,0.4,0.5,0.6,0.7,0.8]\n",
    "clip=np.array([0.2,5.0])\n",
    "step_forward = [1, 2, 3,4,5]\n",
    "sample_size=200\n",
    "action_card=2\n",
    "lr=0.006\n",
    "behavior_lr=0.01\n",
    "beta=0.5\n",
    "batch_size=64\n",
    "tau=0.1\n",
    "\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=18, verbose=1)(\n",
    "    delayed(run_single_experiment)(\n",
    "        i, n_tr, gam, T_obs, seed, n_te, T, num_quantiles, B, alp, eta, step_forward,sample_size,clip,action_card,lr,behavior_lr,beta,batch_size,tau\n",
    "    ) for i in range(rep)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30500b6f-eecd-4a06-81ef-62fd05e6bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb_new_cov_tau02_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_cov_tau03_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_cov_tau04_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_cov_tau05_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_cov_tau06_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_cov_tau07_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_cov_tau08_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_len_tau02_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_len_tau03_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_len_tau04_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_len_tau05_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_len_tau06_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_len_tau07_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "rb_new_len_tau08_e3 = np.zeros((rep, np.shape(step_forward)[0]))\n",
    "#Restore data\n",
    "for i in range(np.shape(results)[0]):\n",
    "    rb_new_cov_tau02_e3[i, :] = results [i][0][0]\n",
    "    rb_new_cov_tau03_e3[i, :] = results [i][0][1]\n",
    "    rb_new_cov_tau04_e3[i, :] = results [i][0][2]\n",
    "    rb_new_cov_tau05_e3[i, :] = results [i][0][3]\n",
    "    rb_new_cov_tau06_e3[i, :] = results [i][0][4]\n",
    "    rb_new_cov_tau07_e3[i, :] = results [i][0][5]\n",
    "    rb_new_cov_tau08_e3[i, :] = results [i][0][6]\n",
    "\n",
    "\n",
    "    rb_new_len_tau02_e3[i, :] = results [i][1][0]\n",
    "    rb_new_len_tau03_e3[i, :] = results [i][1][1]\n",
    "    rb_new_len_tau04_e3[i, :] = results [i][1][2]\n",
    "    rb_new_len_tau05_e3[i, :] = results [i][1][3]\n",
    "    rb_new_len_tau06_e3[i, :] = results [i][1][4]\n",
    "    rb_new_len_tau07_e3[i, :] = results [i][1][5]\n",
    "    rb_new_len_tau08_e3[i, :] = results [i][1][6]\n",
    "    \n",
    "\n",
    "PI_cov_all = [res[0] for res in results]\n",
    "PI_len_all = [res[1] for res in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d5a268c-c57b-4bb0-86e0-ea39bc4150dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "##### save simulation result\n",
    "\n",
    "data_new_rb_cov = {\n",
    "    'new-rb-1': rb_new_cov_tau02_e3[:,0],\n",
    "    'new-rb-2': rb_new_cov_tau02_e3[:,1],\n",
    "    'new-rb-3': rb_new_cov_tau02_e3[:,2],\n",
    "    'new-rb-4': rb_new_cov_tau02_e3[:,3],\n",
    "    'new-rb-5': rb_new_cov_tau02_e3[:,4],\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "data_cov = pd.DataFrame(data_new_rb_cov)\n",
    "data_cov.to_excel('cov_shi_off_02.xlsx', index=False)\n",
    "\n",
    "#df_cov.to_excel('simu_res/res_new_rb_e_cov_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "data_new_rb_len = {\n",
    "     'new-rb-1': rb_new_len_tau02_e3[:,0],\n",
    "     'new-rb-2': rb_new_len_tau02_e3[:,1],\n",
    "     'new-rb-3': rb_new_len_tau02_e3[:,2],\n",
    "     'new-rb-4': rb_new_len_tau02_e3[:,3],\n",
    "     'new-rb-5': rb_new_len_tau02_e3[:,4],\n",
    "\n",
    "}\n",
    "\n",
    "data_len = pd.DataFrame(data_new_rb_len)\n",
    "data_len.to_excel('len_shi_off_02.xlsx', index=False)\n",
    "\n",
    "#df_len.to_excel('simu_res/res_new_rb_e_len_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "168eb5b3-7fdb-46eb-a2d4-a6273491d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_rb_cov = {\n",
    "    'new-rb-1': rb_new_cov_tau03_e3[:,0],\n",
    "    'new-rb-2': rb_new_cov_tau03_e3[:,1],\n",
    "    'new-rb-3': rb_new_cov_tau03_e3[:,2],\n",
    "    'new-rb-4': rb_new_cov_tau03_e3[:,3],\n",
    "    'new-rb-5': rb_new_cov_tau03_e3[:,4],\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "data_cov = pd.DataFrame(data_new_rb_cov)\n",
    "data_cov.to_excel('cov_shi_off_03.xlsx', index=False)\n",
    "\n",
    "#df_cov.to_excel('simu_res/res_new_rb_e_cov_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "data_new_rb_len = {\n",
    "     'new-rb-1': rb_new_len_tau03_e3[:,0],\n",
    "     'new-rb-2': rb_new_len_tau03_e3[:,1],\n",
    "     'new-rb-3': rb_new_len_tau03_e3[:,2],\n",
    "     'new-rb-4': rb_new_len_tau03_e3[:,3],\n",
    "     'new-rb-5': rb_new_len_tau03_e3[:,4],\n",
    "\n",
    "}\n",
    "\n",
    "data_len = pd.DataFrame(data_new_rb_len)\n",
    "data_len.to_excel('len_shi_off_03.xlsx', index=False)\n",
    "\n",
    "#df_len.to_excel('simu_res/res_new_rb_e_len_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1f4e8-5931-4ebd-9c83-8f420b5d2822",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_rb_cov = {\n",
    "    'new-rb-1': rb_new_cov_tau04_e3[:,0],\n",
    "    'new-rb-2': rb_new_cov_tau04_e3[:,1],\n",
    "    'new-rb-3': rb_new_cov_tau04_e3[:,2],\n",
    "    'new-rb-4': rb_new_cov_tau04_e3[:,3],\n",
    "    'new-rb-5': rb_new_cov_tau04_e3[:,4],\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "data_cov = pd.DataFrame(data_new_rb_cov)\n",
    "data_cov.to_excel('cov_shi_off_04.xlsx', index=False)\n",
    "\n",
    "#df_cov.to_excel('simu_res/res_new_rb_e_cov_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "data_new_rb_len = {\n",
    "     'new-rb-1': rb_new_len_tau04_e3[:,0],\n",
    "     'new-rb-2': rb_new_len_tau04_e3[:,1],\n",
    "     'new-rb-3': rb_new_len_tau04_e3[:,2],\n",
    "     'new-rb-4': rb_new_len_tau04_e3[:,3],\n",
    "     'new-rb-5': rb_new_len_tau04_e3[:,4],\n",
    "\n",
    "}\n",
    "\n",
    "data_len = pd.DataFrame(data_new_rb_len)\n",
    "data_len.to_excel('len_shi_off_04.xlsx', index=False)\n",
    "\n",
    "#df_len.to_excel('simu_res/res_new_rb_e_len_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4132f-9f5d-4b76-bfb5-0d5938cec6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_rb_cov = {\n",
    "    'new-rb-1': rb_new_cov_tau05_e3[:,0],\n",
    "    'new-rb-2': rb_new_cov_tau05_e3[:,1],\n",
    "    'new-rb-3': rb_new_cov_tau05_e3[:,2],\n",
    "    'new-rb-4': rb_new_cov_tau05_e3[:,3],\n",
    "    'new-rb-5': rb_new_cov_tau05_e3[:,4],\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "data_cov = pd.DataFrame(data_new_rb_cov)\n",
    "data_cov.to_excel('cov_shi_off_05.xlsx', index=False)\n",
    "\n",
    "#df_cov.to_excel('simu_res/res_new_rb_e_cov_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "data_new_rb_len = {\n",
    "     'new-rb-1': rb_new_len_tau05_e3[:,0],\n",
    "     'new-rb-2': rb_new_len_tau05_e3[:,1],\n",
    "     'new-rb-3': rb_new_len_tau05_e3[:,2],\n",
    "     'new-rb-4': rb_new_len_tau05_e3[:,3],\n",
    "     'new-rb-5': rb_new_len_tau05_e3[:,4],\n",
    "\n",
    "}\n",
    "\n",
    "data_len = pd.DataFrame(data_new_rb_len)\n",
    "data_len.to_excel('len_shi_off_05.xlsx', index=False)\n",
    "\n",
    "#df_len.to_excel('simu_res/res_new_rb_e_len_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6babc-ee7d-48f0-8f8a-abb69268caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_rb_cov = {\n",
    "    'new-rb-1': rb_new_cov_tau06_e3[:,0],\n",
    "    'new-rb-2': rb_new_cov_tau06_e3[:,1],\n",
    "    'new-rb-3': rb_new_cov_tau06_e3[:,2],\n",
    "    'new-rb-4': rb_new_cov_tau06_e3[:,3],\n",
    "    'new-rb-5': rb_new_cov_tau06_e3[:,4],\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "data_cov = pd.DataFrame(data_new_rb_cov)\n",
    "data_cov.to_excel('cov_shi_off_06.xlsx', index=False)\n",
    "\n",
    "#df_cov.to_excel('simu_res/res_new_rb_e_cov_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "data_new_rb_len = {\n",
    "     'new-rb-1': rb_new_len_tau06_e3[:,0],\n",
    "     'new-rb-2': rb_new_len_tau06_e3[:,1],\n",
    "     'new-rb-3': rb_new_len_tau06_e3[:,2],\n",
    "     'new-rb-4': rb_new_len_tau06_e3[:,3],\n",
    "     'new-rb-5': rb_new_len_tau06_e3[:,4],\n",
    "\n",
    "}\n",
    "\n",
    "data_len = pd.DataFrame(data_new_rb_len)\n",
    "data_len.to_excel('len_shi_off_06.xlsx', index=False)\n",
    "\n",
    "#df_len.to_excel('simu_res/res_new_rb_e_len_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab6a1d-1c3d-4685-a4f1-fdcd52d503a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_rb_cov = {\n",
    "    'new-rb-1': rb_new_cov_tau07_e3[:,0],\n",
    "    'new-rb-2': rb_new_cov_tau07_e3[:,1],\n",
    "    'new-rb-3': rb_new_cov_tau07_e3[:,2],\n",
    "    'new-rb-4': rb_new_cov_tau07_e3[:,3],\n",
    "    'new-rb-5': rb_new_cov_tau07_e3[:,4],\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "data_cov = pd.DataFrame(data_new_rb_cov)\n",
    "data_cov.to_excel('cov_shi_off_07.xlsx', index=False)\n",
    "\n",
    "#df_cov.to_excel('simu_res/res_new_rb_e_cov_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "data_new_rb_len = {\n",
    "     'new-rb-1': rb_new_len_tau07_e3[:,0],\n",
    "     'new-rb-2': rb_new_len_tau07_e3[:,1],\n",
    "     'new-rb-3': rb_new_len_tau07_e3[:,2],\n",
    "     'new-rb-4': rb_new_len_tau07_e3[:,3],\n",
    "     'new-rb-5': rb_new_len_tau07_e3[:,4],\n",
    "\n",
    "}\n",
    "\n",
    "data_len = pd.DataFrame(data_new_rb_len)\n",
    "data_len.to_excel('len_shi_off_07.xlsx', index=False)\n",
    "\n",
    "#df_len.to_excel('simu_res/res_new_rb_e_len_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8957e5-df67-41bf-8abd-df2be75a1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_rb_cov = {\n",
    "    'new-rb-1': rb_new_cov_tau08_e3[:,0],\n",
    "    'new-rb-2': rb_new_cov_tau08_e3[:,1],\n",
    "    'new-rb-3': rb_new_cov_tau08_e3[:,2],\n",
    "    'new-rb-4': rb_new_cov_tau08_e3[:,3],\n",
    "    'new-rb-5': rb_new_cov_tau08_e3[:,4],\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "data_cov = pd.DataFrame(data_new_rb_cov)\n",
    "data_cov.to_excel('cov_shi_off_08.xlsx', index=False)\n",
    "\n",
    "#df_cov.to_excel('simu_res/res_new_rb_e_cov_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "data_new_rb_len = {\n",
    "     'new-rb-1': rb_new_len_tau08_e3[:,0],\n",
    "     'new-rb-2': rb_new_len_tau08_e3[:,1],\n",
    "     'new-rb-3': rb_new_len_tau08_e3[:,2],\n",
    "     'new-rb-4': rb_new_len_tau08_e3[:,3],\n",
    "     'new-rb-5': rb_new_len_tau08_e3[:,4],\n",
    "\n",
    "}\n",
    "\n",
    "data_len = pd.DataFrame(data_new_rb_len)\n",
    "data_len.to_excel('len_shi_off_08.xlsx', index=False)\n",
    "\n",
    "#df_len.to_excel('simu_res/res_new_rb_e_len_gam0.8_tau0.5_qnum30_100to200.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f371c-cb83-4120-bccf-26e71151d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_new_rb_cov = {\n",
    "    'new-rb-1': data_new_rb_cov_5.iloc[:,0],\n",
    "    'new-rb-2': data_new_rb_cov_6.iloc[:,1],\n",
    "    'new-rb-3': data_new_rb_cov_7.iloc[:,2],\n",
    "    'new-rb-4': data_new_rb_cov_8.iloc[:,3],\n",
    "    'new-rb-5': data_new_rb_cov_8.iloc[:,4],\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "data_new_rb_cov = pd.DataFrame(data_new_rb_cov)\n",
    "\n",
    "\n",
    "data_new_rb_len = {\n",
    "    'new-rb-1': data_new_rb_len_5.iloc[:,0],\n",
    "    'new-rb-2': data_new_rb_len_6.iloc[:,1],\n",
    "    'new-rb-3': data_new_rb_len_7.iloc[:,2],\n",
    "    'new-rb-4': data_new_rb_len_8.iloc[:,3],\n",
    "    'new-rb-5': data_new_rb_len_8.iloc[:,4],\n",
    "\n",
    "}\n",
    "\n",
    "data_new_rb_len = pd.DataFrame(data_new_rb_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a08941fe-d831-4131-b12b-d2c8c90f7e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:  5.7min finished\n"
     ]
    }
   ],
   "source": [
    "rep = 100\n",
    "n_tr = 200\n",
    "gam = 0.8\n",
    "T_obs = 30\n",
    "seed = 2025\n",
    "n_te = 310\n",
    "T = 70\n",
    "num_quantiles = 30\n",
    "clip=np.array([0.2,5.0])\n",
    "step_forward = [1, 2, 3,4,5]\n",
    "action_card=2\n",
    "lr=0.003\n",
    "behavior_lr=0.005\n",
    "beta=0.5\n",
    "batch_size=64\n",
    "tau=0.1\n",
    "alp=0.1\n",
    "res_quan = np.zeros((rep, 2))\n",
    "\n",
    "# Parallel Calculation\n",
    "def process_iteration(i,n_tr, gam, T_obs, n_te, T, num_quantiles, alp, seed,action_card,lr,behavior_lr,beta,batch_size,tau):\n",
    " \n",
    "    data_train = data_gen(N=n_tr,\n",
    "                         T_obs=T_obs,\n",
    "                         T=T,\n",
    "                         gam=gam,\n",
    "                         seed=seed+i,\n",
    "                         s_init=None)\n",
    "    \n",
    "\n",
    "    data_test = data_gen_new(N=n_te,\n",
    "                            T_obs=1,\n",
    "                            T=T,\n",
    "                            gam=gam,\n",
    "                            seed=seed + i + 10000,\n",
    "                            s_init=None)\n",
    "    \n",
    "\n",
    "    quan_PI_res1 = quantile_region_res(data_train=data_train,\n",
    "                                      data_test=data_test, \n",
    "                                      gam=gam, \n",
    "                                      alp=alp,\n",
    "                                      num_quantiles=num_quantiles,\n",
    "                                      seed=seed+i,\n",
    "                                        action_card=action_card,\n",
    "                                        lr=lr,\n",
    "                                        behavior_lr=behavior_lr,\n",
    "                                        beta=beta,\n",
    "                                        batch_size=batch_size,\n",
    "                                        tau=tau)\n",
    "    \n",
    "    print(f\"test num: {i}\")\n",
    "    print(\"quantile region: \")\n",
    "    print(f\"cov: {quan_PI_res1[0]} | length: {quan_PI_res1[1]}\")\n",
    "    \n",
    "    return quan_PI_res1\n",
    "\n",
    "\n",
    "\n",
    "results_qr = Parallel(n_jobs=18, verbose=1)(delayed(process_iteration)(i, n_tr, gam, T_obs, n_te, T, num_quantiles, alp, seed,action_card,lr,behavior_lr,beta,batch_size,tau) for i in range(rep))\n",
    "\n",
    "# restore data\n",
    "for i in range(rep):\n",
    "    res_quan[i, :] = results_qr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b055a675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T07:47:46.618942Z",
     "start_time": "2025-03-19T07:47:46.550210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   quantile region\n",
      "0         0.832258\n",
      "1         0.845161\n",
      "2         0.829032\n",
      "3         0.854839\n",
      "4         0.822581\n",
      "   quantile region\n",
      "0         8.165129\n",
      "1         8.385649\n",
      "2         8.284308\n",
      "3         8.543129\n",
      "4         8.108450\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "##### save simulation result\n",
    "data_quan_cov = {\n",
    "    'quantile region': res_quan[:, 0]\n",
    "}\n",
    "\n",
    "df_cov = pd.DataFrame(data_quan_cov)\n",
    "\n",
    "df_cov.to_excel('QR_cov_shi_off.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "data_quan_len = {\n",
    "    'quantile region': res_quan[:, 1]\n",
    "}\n",
    "\n",
    "df_len = pd.DataFrame(data_quan_len)\n",
    "\n",
    "df_len.to_excel('QR_len_shi_off.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "print(df_cov.head())\n",
    "print(df_len.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35a57b97-7f13-49c3-be61-66f95ed93253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantile region: \n",
      "coverage probability:  0.8309677419354837 |  average length:  8.191806151636186\n"
     ]
    }
   ],
   "source": [
    "print(\"quantile region: \")\n",
    "print(\"coverage probability: \", np.mean(res_quan[:, 0]),\n",
    "      \"|  average length: \", np.mean(res_quan[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5d3be7",
   "metadata": {},
   "source": [
    "# Plotting simulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7c59d47-218e-42d2-81d3-b36efdabaf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   new-rb-1  new-rb-2  new-rb-3  new-rb-4  new-rb-5        QR\n",
      "0  0.877419  0.877419  0.877419  0.877419  0.877419  0.887097\n",
      "1  0.880645  0.880645  0.880645  0.883871  0.890323  0.864516\n",
      "2  0.883871  0.887097  0.887097  0.883871  0.883871  0.887097\n",
      "3  0.877419  0.877419  0.877419  0.877419  0.883871  0.887097\n",
      "4  0.877419  0.880645  0.880645  0.880645  0.887097  0.887097\n",
      "   new-rb-1  new-rb-2  new-rb-3  new-rb-4  new-rb-5        QR\n",
      "0  7.184103  7.300162  7.227494  7.234046  7.354492  7.398871\n",
      "1  7.163068  7.181728  7.286160  7.319855  7.422460  7.187608\n",
      "2  7.292234  7.425412  7.465158  7.339162  7.246481  7.400118\n",
      "3  7.208743  7.352818  7.360902  7.363134  7.422255  7.383613\n",
      "4  7.155576  7.290999  7.305147  7.293528  7.457363  7.438672\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data_new_rb_cov.columns = ['k=1', 'k=2', 'k=3' , 'k=4' , 'k=5']\n",
    "data_new_rb_len.columns = ['k=1', 'k=2', 'k=3' , 'k=4' , 'k=5']\n",
    "\n",
    "data_QR_cov = pd.read_excel('QR_cov_shi_off.xlsx')\n",
    "data_QR_len = pd.read_excel('QR_len_shi_off.xlsx')\n",
    "\n",
    "\n",
    "#data_new_rb_cov.rename(columns={'quantile region': 'QR'}, inplace=True)\n",
    "#data_new_rb_len.rename(columns={'quantile region': 'QR'}, inplace=True)\n",
    "data_new_rb_cov['DRL-QR'] = data_QR_cov['quantile region']\n",
    "data_new_rb_len['DRL-QR'] = data_QR_len['quantile region']\n",
    "\n",
    "print(data_new_rb_cov.head())\n",
    "print(data_new_rb_len.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cca2af0-8fd7-48ae-95bb-e8dee6f2173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "bplot_new_cov = data_new_rb_cov.boxplot(patch_artist=True,\n",
    "                         medianprops={\n",
    "                             'linestyle': '-',\n",
    "                             'color': 'black',\n",
    "                             'linewidth': 1.5\n",
    "                         },\n",
    "                         whiskerprops={\n",
    "                             'linestyle': '--',\n",
    "                             'color': 'black'\n",
    "                         },\n",
    "                         capprops={\n",
    "                             'linestyle': '-',\n",
    "                             'color': 'black'\n",
    "                         },\n",
    "                         boxprops={\n",
    "                             'linestyle': '-',\n",
    "                             'color': 'black'\n",
    "                         })\n",
    "\n",
    "colors = [\n",
    "    'goldenrod', 'orange', 'gold', 'khaki', 'wheat', 'lightyellow','skyblue'\n",
    "]\n",
    "\n",
    "colors2 = [\n",
    "    'darkseagreen','limegreen' ,'greenyellow','yellowgreen','lightgreen','honeydew','skyblue'\n",
    "]\n",
    "for patch, color in zip(bplot_new_cov.patches, colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_linewidth(1)\n",
    "\n",
    "bplot_new_cov.yaxis.grid(False)\n",
    "bplot_new_cov.xaxis.grid(False)\n",
    "bplot_new_cov.set_xlabel(\"Method\")\n",
    "bplot_new_cov.set_ylabel(\"Coverage Probability\")\n",
    "\n",
    "plt.axhline(y=0.90, color='red', linestyle='-', linewidth=1)\n",
    "plt.ylim(0.75,1)\n",
    "#plt.savefig('fig/new_rb_cov_o_gam0.8_nr100_qnum10.png')\n",
    "plt.show()\n",
    "plt.savefig('Ex2_off_policy_cp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8cbe52-4f5c-46a4-a285-9f4e2bf3db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "bplot_new_len = data_new_rb_len.boxplot(patch_artist=True,\n",
    "                         medianprops={\n",
    "                             'linestyle': '-',\n",
    "                             'color': 'black',\n",
    "                             'linewidth': 1.5\n",
    "                         },\n",
    "                         whiskerprops={\n",
    "                             'linestyle': '--',\n",
    "                             'color': 'black'\n",
    "                         },\n",
    "                         capprops={\n",
    "                             'linestyle': '-',\n",
    "                             'color': 'black'\n",
    "                         },\n",
    "                         boxprops={\n",
    "                             'linestyle': '-',\n",
    "                             'color': 'black'\n",
    "                         })\n",
    "\n",
    "\n",
    "for patch, color in zip(bplot_new_len.patches, colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_linewidth(1)\n",
    "\n",
    "bplot_new_len.yaxis.grid(False)\n",
    "bplot_new_len.xaxis.grid(False)\n",
    "bplot_new_len.set_xlabel(\"Method\")\n",
    "bplot_new_len.set_ylabel(\"Empirical Length\")\n",
    "#plt.savefig('fig/new_len_o_gam0.8_nr100_qnum10.png')\n",
    "plt.show()\n",
    "plt.savefig('Ex2_off_policy_al.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
